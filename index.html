#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
Izu Coastal Radar -- 伊豆太陽ホーム scraper (FINAL CLEAN VERSION)
- Fixes the TypeError and ensures clean data output.
- Prioritizes property photos.
"""

import json
import re
from datetime import datetime
from urllib.parse import urljoin
import time

import requests
from bs4 import BeautifulSoup

BASE_DOMAIN = "https://www.izutaiyo.co.jp"

FALLBACK_FX_RATE = 156.69

ENTRY_URLS = [
    "https://www.izutaiyo.co.jp/tokusen.php?-Token=True&hpkind=0&hpno=SMB410H+SMB408M+SM2013M+KWB235H+MI2001H+SMB407M+SMB409H+SMB391H+SMB342G+SMB392H+SM2009H+SM2010H+SM2008H+SM2003H+MIB375H+&hptantou=shimoda"
]

JAPANESE_CITIES = {
    "下田市": "Shimoda City",
    "伊東市": "Ito City",
    "東伊豆町": "Higashi-Izu Town",
    "河津町": "Kawazu Town",
    "南伊豆町": "Minami-Izu Town",
}

# --- FX Rate and Helpers ---
def fetch_current_fx_rate() -> float:
    fx_url = "https://api.exchangerate-api.com/v4/latest/USD"
    try:
        response = requests.get(fx_url, timeout=5)
        response.raise_for_status() 
        return float(response.json()['rates'].get('JPY', FALLBACK_FX_RATE))
    except Exception:
        return FALLBACK_FX_RATE

def text_clean(s: str) -> str:
    if not s: return ""
    return re.sub(r"\s+", " ", s).strip()

def to_float(x):
    if x is None: return None
    s = str(x).strip().replace(",", "")
    try: return float(s)
    except Exception: return None

def to_int(x):
    if x is None: return None
    s = str(x).strip().replace(",", "")
    try: return int(float(s))
    except Exception: return None

def translate_title_to_en(jp_title: str) -> str:
    t = jp_title
    for jp_city, en_city in JAPANESE_CITIES.items():
        t = t.replace(jp_city, en_city.replace(" City", "").replace(" Town", ""))
    t = t.replace("海一望", "Ocean View").replace("海遠望", "Distant Sea View").replace("土地", "Land")
    return text_clean(t) or jp_title

def compute_sea_view_score(text: str) -> int | None:
    if any(k in text for k in ["オーシャンフロント", "海一望", "海が目の前"]): return 5
    if any(k in text for k in ["オーシャンビュー", "海望む", "海が見えます", "海が見える"]): return 4
    if "海遠望" in text: return 3
    if "海" in text and "見えない" not in text and "不可" not in text: return 2
    return None

# --- Image Handling ---
SKIP_FRAGMENTS = ["/img/ico", "icon", "logo", "noimage", "header", "menu"]
PHOTO_FRAGMENTS = ["/bb/", "bukken", "photo", "gaikan", "room", "外観", "内観"]

def pick_best_image(soup: BeautifulSoup) -> str | None:
    imgs = soup.find_all("img")
    best_src, best_score = None, -1
    for img in imgs:
        src = img.get("src")
        if not src: continue
        lower = src.lower()
        if any(b in lower for b in SKIP_FRAGMENTS): continue
        if any(k in lower for k in ["madori", "間取", "間取り"]): continue 
        score = 0
        if any(g in lower for g in PHOTO_FRAGMENTS): score += 300
        w = to_float(img.get("width") or "0") or 0
        h = to_float(img.get("height") or "0") or 0
        if w > 200 and h > 150: score += (w * h) / 100.0
        else: score += 50
        if score > best_score: best_score, best_src = score, src
    
    if not best_src: return None
    return urljoin(BASE_DOMAIN, best_src)

def find_property_image(detail_soup: BeautifulSoup) -> str | None:
    # Prioritizing image directly from the detail page (d.php)
    best_image = pick_best_image(detail_soup)
    if best_image:
        return best_image
    return None

def parse_year_built(text: str) -> int | None:
    # Age disabled
    return None

# --- Main Parsing Loop ---
def parse_detail_page(detail_url: str) -> dict | None:
    try:
        res = requests.get(detail_url, timeout=20)
        soup = BeautifulSoup(res.text, "html.parser")
        full_text = soup.get_text(" ", strip=True)
        
        # --- Data Extraction ---
        title = text_clean(soup.find("h2").get_text()) if soup.find("h2") else "物件"
        price_jpy = to_float(re.search(r"([0-9,]+)\s*万円", full_text).group(1)) * 10000 if re.search(r"([0-9,]+)\s*万円", full_text) else None
        land_sqm = to_float(re.search(r"(?:土地地積|地積|土地面積)[:：]\s*([\d,\.]+)", full_text).group(1)) if re.search(r"(?:土地地積|地積|土地面積)[:：]\s*([\d,\.]+)", full_text) else None
        building_sqm = to_float(re.search(r"(?:床面積|建物).*?([\d,\.]+)\s*㎡", full_text).group(1)) if re.search(r"(?:床面積|建物).*?([\d,\.]+)\s*㎡", full_text) else None
        sea_view_score = compute_sea_view_score(full_text)
        
        city = next((jp for jp in JAPANESE_CITIES if jp in full_text), "下田市")
        property_type = "land" if not building_sqm and land_sqm else ("mansion" if any(k in title for k in ["マンション", "アーク", "リゾート"]) else "house")
        
        # --- Tagging: FIX for TypeError ---
        highlightTags = [property_type.capitalize()]
        
        # Check if score is an integer (not None) before comparison
        if isinstance(sea_view_score, int) and sea_view_score >= 4:
            highlightTags.append("Sea view")
            
        listing = {
            "id": f"izutaiyo-{hash(detail_url)}",
            "sourceUrl": detail_url,
            "title": title,
            "titleEn": translate_title_to_en(title),
            "propertyType": property_type,
            "city": city,
            "priceJpy": price_jpy,
            "landSqm": land_sqm,
            "buildingSqm": building_sqm,
            "yearBuilt": None,
            "seaViewScore": sea_view_score,
            "imageUrl": find_property_image(soup),
            "highlightTags": highlightTags,
        }
        return listing
    except Exception:
        # Catch-all to prevent script crash and allow the other listings to be processed
        return None

def parse_recommended_page(url: str) -> list[dict]:
    try:
        res = requests.get(url, timeout=20)
        soup = BeautifulSoup(res.text, "html.parser")
        detail_links = sorted(set([urljoin(BASE_DOMAIN + "/", a["href"]) for a in soup.find_all("a", href=True) if "d.php?hpno=" in a["href"]]))
    except Exception:
        return []
    
    listings = []
    for durl in detail_links:
        # Time.sleep is inside the loop where it belongs
        time.sleep(0.3) 
        l = parse_detail_page(durl)
        if l:
            listings.append(l)
    return listings

def main():
    listings = list({l["id"]: l for url in ENTRY_URLS for l in parse_recommended_page(url)}.values())
    
    FX_RATE = fetch_current_fx_rate()
    prices = [l['priceJpy'] for l in listings if l['priceJpy'] is not None]
    buildings = [l['buildingSqm'] for l in listings if l['buildingSqm'] is not None]

    stats = {
        "avgPriceM": (sum(prices) / len(prices)) / 1000000 if prices else 0,
        "avgBuildingSqm": sum(buildings) / len(buildings) if buildings else 0,
        "avgAge": 0
    }
    
    out = {"fxRate": FX_RATE, "generatedAt": datetime.utcnow().isoformat() + "Z", "listings": listings, "stats": stats}

    with open("listings.json", "w", encoding="utf-8") as f:
        json.dump(out, f, ensure_ascii=False, indent=2)
        
    print(f"Successfully scraped {len(listings)} listings and saved to listings.json")

if __name__ == "__main__":
    import requests
    main()
